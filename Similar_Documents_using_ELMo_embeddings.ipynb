{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similar Documents using ELMo embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeg7n2dT/+QMuu+0tzN2F7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDKfOel6BqjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94df148-eb47-4d9a-afe9-1d3a357ab031"
      },
      "source": [
        "# Using TF Hub requires particular version of TF\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.7.0\n",
            "Uninstalling tensorflow-2.7.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.7.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.7.0\n",
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 22 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 21.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.13.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.42.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=2d69aace5ae3c1ebdfea70de4148c6d95ed1066047a02d9e7548addc255f98da\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0LHX-CtDWSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0a4ce3-f4d3-48cd-e3f6-165d3ffc30e4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ3XM2IkJWBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfca78eb-8b9c-4565-dad4-2f2e4ff5de28"
      },
      "source": [
        "# Load the file to your google Drive\n",
        "# data in sgm can be downloaded from below link:\n",
        "# http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "import os \n",
        "\n",
        "\n",
        "documents = []\n",
        "for file in os.listdir(\"/content/drive/My Drive/reuters_data/\"):\n",
        "    \n",
        "    if file.endswith(\".sgm\"):\n",
        "        \n",
        "        # for each sgm file, read it\n",
        "        # Update your file path below if you are mounting for google drive\n",
        "        # It will ask for gmail login id and password details\n",
        "        filename = os.path.join(\"/content/drive/My Drive/reuters_data/\", file)\n",
        "        f = open(filename, 'r', encoding='utf-8', errors='ignore')\n",
        "        dataFile = f.read()\n",
        "        \n",
        "        # pass it to BeautifulSoup\n",
        "        soup = BeautifulSoup(dataFile, 'html.parser')\n",
        "        contents = soup.findAll('body')\n",
        "        \n",
        "        # for each body tag, extract it's text\n",
        "        for content in contents:\n",
        "            documents.append(content.text)\n",
        "print('We have {} documents'.format(len(documents)))\n",
        "print(documents[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "We have 19043 documents\n",
            "Showers continued throughout the week in\n",
            "the Bahia cocoa zone, alleviating the drought since early\n",
            "January and improving prospects for the coming temporao,\n",
            "although normal humidity levels have not been restored,\n",
            "Comissaria Smith said in its weekly review.\n",
            "    The dry period means the temporao will be late this year.\n",
            "    Arrivals for the week ended February 22 were 155,221 bags\n",
            "of 60 kilos making a cumulative total for the season of 5.93\n",
            "mln against 5.81 at the same stage last year. Again it seems\n",
            "that cocoa delivered earlier on consignment was included in the\n",
            "arrivals figures.\n",
            "    Comissaria Smith said there is still some doubt as to how\n",
            "much old crop cocoa is still available as harvesting has\n",
            "practically come to an end. With total Bahia crop estimates\n",
            "around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
            "are a few hundred thousand bags still in the hands of farmers,\n",
            "middlemen, exporters and processors.\n",
            "    There are doubts as to how much of this cocoa would be fit\n",
            "for export as shippers are now experiencing dificulties in\n",
            "obtaining +Bahia superior+ certificates.\n",
            "    In view of the lower quality over recent weeks farmers have\n",
            "sold a good part of their cocoa held on consignment.\n",
            "    Comissaria Smith said spot bean prices rose to 340 to 350\n",
            "cruzados per arroba of 15 kilos.\n",
            "    Bean shippers were reluctant to offer nearby shipment and\n",
            "only limited sales were booked for March shipment at 1,750 to\n",
            "1,780 dlrs per tonne to ports to be named.\n",
            "    New crop sales were also light and all to open ports with\n",
            "June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
            "under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\n",
            "per tonne FOB.\n",
            "    Routine sales of butter were made. March/April sold at\n",
            "4,340, 4,345 and 4,350 dlrs.\n",
            "    April/May butter went at 2.27 times New York May, June/July\n",
            "at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\n",
            "2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
            "2.27 times New York Dec, Comissaria Smith said.\n",
            "    Destinations were the U.S., Covertible currency areas,\n",
            "Uruguay and open ports.\n",
            "    Cake sales were registered at 785 to 995 dlrs for\n",
            "March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
            "New York Dec for Oct/Dec.\n",
            "    Buyers were the U.S., Argentina, Uruguay and convertible\n",
            "currency areas.\n",
            "    Liquor sales were limited with March/April selling at 2,325\n",
            "and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
            "York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
            "Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
            "said.\n",
            "    Total Bahia sales are currently estimated at 6.13 mln bags\n",
            "against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
            "crop.\n",
            "    Final figures for the period to February 28 are expected to\n",
            "be published by the Brazilian Cocoa Trade Commission after\n",
            "carnival which ends midday on February 27.\n",
            " Reuter\n",
            "\u0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWcfnTnNSLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f8623753-6f65-4344-9df9-d4b53acb2ba9"
      },
      "source": [
        "import re\n",
        "\n",
        "def removeLinks(text):\n",
        "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "def removeHTMLTags(text):\n",
        "    text = re.sub(r'<.*?>', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def convertToLowerCase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def removeWordsWithNumbers(text):\n",
        "    return re.sub(r\"\\S*\\d\\S*\", \"\", text).strip()\n",
        "\n",
        "def removePuctuations(text):\n",
        "    return re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
        "\n",
        "def removeWhiteSpaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "documents = list(map(removeLinks, documents))\n",
        "documents = list(map(convertToLowerCase, documents))\n",
        "documents = list(map(removeWordsWithNumbers, documents))\n",
        "documents = list(map(removeHTMLTags, documents))\n",
        "documents = list(map(decontracted, documents))\n",
        "documents = list(map(removePuctuations, documents))\n",
        "documents = list(map(removeWhiteSpaces, documents))\n",
        "documents[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'showers continued throughout the week in the bahia cocoa zone alleviating the drought since early january and improving prospects for the coming temporao although normal humidity levels have not been restored comissaria smith said in its weekly review the dry period means the temporao will be late this year arrivals for the week ended february were bags of kilos making a cumulative total for the season of mln against at the same stage last year again it seems that cocoa delivered earlier on consignment was included in the arrivals figures comissaria smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end with total bahia crop estimates around mln bags and sales standing at almost mln there are a few hundred thousand bags still in the hands of farmers middlemen exporters and processors there are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining bahia superior certificates in view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment comissaria smith said spot bean prices rose to to cruzados per arroba of kilos bean shippers were reluctant to offer nearby shipment and only limited sales were booked for march shipment at to dlrs per tonne to ports to be named new crop sales were also light and all to open ports with june july going at and dlrs and at and dlrs under new york july aug sept at and dlrs per tonne fob routine sales of butter were made march april sold at and dlrs april may butter went at times new york may june july at and dlrs aug sept at to dlrs and at and times new york sept and oct dec at dlrs and times new york dec comissaria smith said destinations were the u s covertible currency areas uruguay and open ports cake sales were registered at to dlrs for march april dlrs for may dlrs for aug and times new york dec for oct dec buyers were the u s argentina uruguay and convertible currency areas liquor sales were limited with march april selling at and dlrs june july at dlrs and at times new york july aug sept at dlrs and at times new york sept and oct dec at times new york dec comissaria smith said total bahia sales are currently estimated at mln bags against the crop and mln bags against the crop final figures for the period to february are expected to be published by the brazilian cocoa trade commission after carnival which ends midday on february reuter'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A--Gp_1MkJI"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u0jRTgDS5FT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e88a4f6-aae4-455a-9498-c5f22442a768"
      },
      "source": [
        "# Uses GPU or CPU?\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nboc4WhrLaic"
      },
      "source": [
        "def elmo_vectors(x):\n",
        "  embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # return average of ELMo features\n",
        "    return sess.run(tf.reduce_mean(embeddings,1))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCWH7UGEO5KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c23121-e5d7-43f3-d94c-4fc2ef121bf4"
      },
      "source": [
        "%%time\n",
        "# Restricting to 10 docs to enable faster computation\n",
        "elmo_docs = [elmo_vectors([x]) for x in documents[:10]]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 8s, sys: 3.62 s, total: 2min 11s\n",
            "Wall time: 1min 21s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDR7X0UlTpW9"
      },
      "source": [
        "# Optional: Save the Embeddings if you want to avoid re-computation\n",
        "import pickle\n",
        "pickle_out = open(\"/content/drive/My Drive/reuters_data/elmo_train_reuters.pickle\",\"wb\")\n",
        "pickle.dump(elmo_docs, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHqg1yIYTwyM"
      },
      "source": [
        "# Optional: Load the saved embeddings\n",
        "import pickle\n",
        "pickle_in = open(\"/content/drive/My Drive/reuters_data/elmo_train_reuters.pickle\", \"rb\")\n",
        "elmo_docs_loaded = pickle.load(pickle_in)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQHlvXVOVGUS"
      },
      "source": [
        "# Find cosine similarity score\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "\n",
        "def find_cosine_score(x1, x2):\n",
        "  return np.dot(x1, x2.T) / LA.norm(x2)\n",
        "\n",
        "# Find all similar documents for the index = '0' \n",
        "doc_idx = 0\n",
        "docA = elmo_docs[doc_idx]\n",
        "docA = docA/LA.norm(docA)\n",
        "\n",
        "sim_score = {}\n",
        "for i in range(len(elmo_docs)):\n",
        "  docB = elmo_docs[i]\n",
        "  sim_score[i] = find_cosine_score(docA, docB) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGP62-8daHOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec537e49-c4a4-4e9a-aa49-75934daed920"
      },
      "source": [
        "# Sort in Descending scores\n",
        "x = sorted(sim_score.items(), key=lambda x:x[1], reverse=True)[:5]\n",
        "x"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, array([[1.]], dtype=float32)),\n",
              " (5, array([[0.8490526]], dtype=float32)),\n",
              " (4, array([[0.7933567]], dtype=float32)),\n",
              " (7, array([[0.7454589]], dtype=float32)),\n",
              " (3, array([[0.74164486]], dtype=float32))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Zwxhghmm-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397640c8-a22d-42c5-f771-1f7efc925244"
      },
      "source": [
        "# Print the Results\n",
        "for i in range(len(x)):\n",
        "  print(f\"-------------Document {i}-----------------\") \n",
        "  print(documents[x[i][0]])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Document 0-----------------\n",
            "showers continued throughout the week in the bahia cocoa zone alleviating the drought since early january and improving prospects for the coming temporao although normal humidity levels have not been restored comissaria smith said in its weekly review the dry period means the temporao will be late this year arrivals for the week ended february were bags of kilos making a cumulative total for the season of mln against at the same stage last year again it seems that cocoa delivered earlier on consignment was included in the arrivals figures comissaria smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end with total bahia crop estimates around mln bags and sales standing at almost mln there are a few hundred thousand bags still in the hands of farmers middlemen exporters and processors there are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining bahia superior certificates in view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment comissaria smith said spot bean prices rose to to cruzados per arroba of kilos bean shippers were reluctant to offer nearby shipment and only limited sales were booked for march shipment at to dlrs per tonne to ports to be named new crop sales were also light and all to open ports with june july going at and dlrs and at and dlrs under new york july aug sept at and dlrs per tonne fob routine sales of butter were made march april sold at and dlrs april may butter went at times new york may june july at and dlrs aug sept at to dlrs and at and times new york sept and oct dec at dlrs and times new york dec comissaria smith said destinations were the u s covertible currency areas uruguay and open ports cake sales were registered at to dlrs for march april dlrs for may dlrs for aug and times new york dec for oct dec buyers were the u s argentina uruguay and convertible currency areas liquor sales were limited with march april selling at and dlrs june july at dlrs and at times new york july aug sept at dlrs and at times new york sept and oct dec at times new york dec comissaria smith said total bahia sales are currently estimated at mln bags against the crop and mln bags against the crop final figures for the period to february are expected to be published by the brazilian cocoa trade commission after carnival which ends midday on february reuter\n",
            "-------------Document 1-----------------\n",
            "argentine grain board figures show crop registrations of grains oilseeds and their products to february in thousands of tonnes showing those for future shipments month total and total to february in brackets bread wheat prev feb march total maize mar total nil sorghum nil nil oilseed export registrations were sunflowerseed total soybean may total nil the board also detailed export registrations for subproducts as follows subproducts wheat prev feb march apr total linseed prev feb mar apr total soybean prev feb mar nil apr nil may total sunflowerseed prev feb mar apr total vegetable oil registrations were sunoil prev feb mar apr may nil jun total linoil prev feb mar apr total soybean oil prev feb mar nil apr may jun jul total reuter\n",
            "-------------Document 2-----------------\n",
            "the u s agriculture department reported the farmer owned reserve national five day average price through february as follows dlrs bu sorghum cwt natl loan release call avge rate x level price price wheat iv v vi corn iv v x rates natl loan release call avge rate x level price price oats v barley n a iv v sorghum iv v reserves i ii and iii have matured level iv reflects grain entered after oct for feedgrain and after july for wheat level v wheat barley after corn sorghum after level vi covers wheat entered after january rates y dlrs per cwt lbs n a not available reuter\n",
            "-------------Document 3-----------------\n",
            "moody is investors service inc said it lowered the debt and preferred stock ratings of usx corp and its units about seven billion dlrs of securities is affected moody is said marathon oil co is recent establishment of up to one billion dlrs in production payment facilities on its prolific yates field has significant negative implications for usx is unsecured creditors the company appears to have positioned its steel segment for a return to profit by late moody is added ratings lowered include those on usx is senior debt to from reuter\n",
            "-------------Document 4-----------------\n",
            "bankamerica corp is not under pressure to act quickly on its proposed equity offering and would do well to delay it because of the stock is recent poor performance banking analysts said some analysts said they have recommended bankamerica delay its up to one billion dlr equity offering which has yet to be approved by the securities and exchange commission bankamerica stock fell this week along with other banking issues on the news that brazil has suspended interest payments on a large portion of its foreign debt the stock traded around down this afternoon after falling to earlier this week on the news banking analysts said that with the immediate threat of the first interstate bancorp takeover bid gone bankamerica is under no pressure to sell the securities into a market that will be nervous on bank stocks in the near term bankamerica filed the offer on january it was seen as one of the major factors leading the first interstate withdrawing its takeover bid on february a bankamerica spokesman said sec approval is taking longer than expected and market conditions must now be re evaluated the circumstances at the time will determine what we do said arthur miller bankamerica is vice president for financial communications when asked if bankamerica would proceed with the offer immediately after it receives sec approval i would put it off as long as they conceivably could said lawrence cohn analyst with merrill lynch pierce fenner and smith cohn said the longer bankamerica waits the longer they have to show the market an improved financial outlook although bankamerica has yet to specify the types of equities it would offer most analysts believed a convertible preferred stock would encompass at least part of it such an offering at a depressed stock price would mean a lower conversion price and more dilution to bankamerica stock holders noted daniel williams analyst with sutro group several analysts said that while they believe the brazilian debt problem will continue to hang over the banking industry through the quarter the initial shock reaction is likely to ease over the coming weeks nevertheless bankamerica which holds about billion dlrs in brazilian loans stands to lose mln dlrs if the interest rate is reduced on the debt and as much as mln dlrs if brazil pays no interest for a year said joseph arsenio analyst with birr wilson and co he noted however that any potential losses would not show up in the current quarter with other major banks standing to lose even more than bankamerica if brazil fails to service its debt the analysts said they expect the debt will be restructured similar to way mexico is debt was minimizing losses to the creditor banks reuter\n"
          ]
        }
      ]
    }
  ]
}